{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97619ab",
   "metadata": {},
   "source": [
    "# ETL Process Labor Market "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74a8ce",
   "metadata": {},
   "source": [
    "##### The purpose of this python script is to scrape web data from multiple websites all related to job postings, salaries and other important data points regarding the labor market in tech. The data collected from this notebook will then be loaded into CSV files which will be analyzed and cleaned in another script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b3b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neccessary libraries for webscraping + data ETL\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57425a55",
   "metadata": {},
   "source": [
    "### This function extracts data from a website and parses it into HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75027807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the webpage that we want to parse\n",
    "def extract(page1, page2=None):\n",
    "    headers = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari\"}\n",
    "    url = f\"{page1}={page2}\"\n",
    "    if page2 == None:\n",
    "        url = f\"{page1}\"\n",
    "    r = requests.get(url, headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fce5c9",
   "metadata": {},
   "source": [
    "### The functions 'transform_2'  & 'parse_indeed' are used to take our unparsed html and convert it into a role variable that contains multiple data points based on a job posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1706e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transofmring our raw html data into a readable format\n",
    "def transform_2(soup, destination):\n",
    "    div = soup.find_all('div', class_ = 'slider_container')\n",
    "    for i in div: \n",
    "        title = i.find('div', class_= 'heading4 color-text-primary singleLineTitle tapItem-gutter').text.strip()\n",
    "        name = i.find('span', class_ = \"companyName\").text.strip()\n",
    "        location = i.find('div', class_ = \"heading6 company_location tapItem-gutter companyInfo\").text.strip()\n",
    "        rating = i.find('span', class_ = \"ratingNumber\").text.strip()\n",
    "        summary = i.find('div', class_= 'job-snippet').text.strip()\n",
    "        \n",
    "        #loading our new variables into a dictionary\n",
    "        role = {\n",
    "            'Title': title,\n",
    "            'Company_Name': name,\n",
    "            'Location': location,\n",
    "            'Rating': rating,\n",
    "            'Summary': summary\n",
    "        } \n",
    "        destination.append(role)\n",
    "\n",
    "#this function parses data from indeed and places it inbto a df        \n",
    "def parse_indeed(company, destination, search1=None, search2=None):\n",
    "        c = extract(f\"https://www.indeed.com/jobs?q={search1}%20{search2}%20{company}&vjk=877ae4112485d7c7&start\", 0)\n",
    "        for i in range(0, 100, 10):\n",
    "            x = transform_2(c, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55cb1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading our nationwide data into a pandas df\n",
    "nationwide_data_analytics = []\n",
    "parse_indeed(\"Nationwide\", nationwide_data_analytics, \"Data\", \"Analytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46e5df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "nationwide_swe = []\n",
    "parse_indeed(\"Nationwide\", nationwide_swe, \"Software\", \"Engineer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b875930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nationwide_data_analytics = pd.DataFrame(nationwide_data_analytics)\n",
    "nationwide_swe = pd.DataFrame(nationwide_swe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60099d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling data from wikipedia list of top 100 technology comapnies, for further data collection\n",
    "r = requests.get(\"https://en.wikipedia.org/wiki/List_of_largest_Internet_companies\")\n",
    "df_list = pd.read_html(r.text) # this parses all the tables in webpages to a list\n",
    "df = df_list[1]\n",
    "top_100 = df['Company']\n",
    "top_100.at[1]='Google'\n",
    "arr = np.array(top_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f490fc",
   "metadata": {},
   "source": [
    "### These few cells loop through our array of the top_100 tech companies by revenue in order to search for various roles that being Analytics and SWE jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57e1b2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n"
     ]
    }
   ],
   "source": [
    "#processes our parse_indeed function for each, name in the top_100 companies\n",
    "all_frames = []\n",
    "\n",
    "for i in arr:\n",
    "    try:\n",
    "        parse_indeed(i, all_frames, 'Data', 'Analytics')\n",
    "    except AttributeError:\n",
    "        print('Ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0b44d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n"
     ]
    }
   ],
   "source": [
    "all_swe = []\n",
    "for i in arr:\n",
    "    try:\n",
    "        parse_indeed(i, all_swe, 'Software', 'Engineer')\n",
    "    except AttributeError:\n",
    "        print('Ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51dc3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analytics_all = pd.DataFrame(all_frames)\n",
    "swe_all = pd.DataFrame(all_swe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a364af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finishing our Indeed ETL Proces by loading our data into a CSV format for later analysis\n",
    "\"\"\"\n",
    "nationwide_data_analytics.to_csv('nationwide_data_analytics.csv')\n",
    "nationwide_swe.to_csv('nationwide_swe.csv')\n",
    "data_analytics_all.to_csv('data_analytics_all.csv')\n",
    "swe_all.to_csv('swe_all.csv')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
