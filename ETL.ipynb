{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094014f1",
   "metadata": {},
   "source": [
    "# ETL Process Labor Market "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63826ca2",
   "metadata": {},
   "source": [
    "##### The purpose of this python script is to scrape web data from multiple websites all related to job postings, salaries and other important data points regarding the labor market in tech. The data collected from this notebook will then be loaded into CSV files which will be analyzed and cleaned in another script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b3b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neccessary libraries for webscraping + data ETL\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77799b53",
   "metadata": {},
   "source": [
    "### This function extracts data from a website and parses it into HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75027807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the webpage that we want to parse\n",
    "def extract(page1, page2=None):\n",
    "    headers = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari\"}\n",
    "    url = f\"{page1}={page2}\"\n",
    "    if page2 == None:\n",
    "        url = f\"{page1}\"\n",
    "    r = requests.get(url, headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeccf0a5",
   "metadata": {},
   "source": [
    "### The functions 'transform_2'  & 'parse_indeed' are used to take our unparsed html and convert it into a role variable that contains multiple data points based on a job posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1706e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transofmring our raw html data into a readable format\n",
    "def transform_2(soup, destination):\n",
    "    div = soup.find_all('div', class_ = 'slider_container')\n",
    "    for i in div: \n",
    "        title = i.find('div', class_= 'heading4 color-text-primary singleLineTitle tapItem-gutter').text.strip()\n",
    "        name = i.find('span', class_ = \"companyName\").text.strip()\n",
    "        location = i.find('div', class_ = \"heading6 company_location tapItem-gutter companyInfo\").text.strip()\n",
    "        rating = i.find('span', class_ = \"ratingNumber\").text.strip()\n",
    "        summary = i.find('div', class_= 'job-snippet').text.strip()\n",
    "        \n",
    "        #loading our new variables into a dictionary\n",
    "        role = {\n",
    "            'Title': title,\n",
    "            'Company_Name': name,\n",
    "            'Location': location,\n",
    "            'Rating': rating,\n",
    "            'Summary': summary\n",
    "        } \n",
    "        destination.append(role)\n",
    "\n",
    "#this function parses data from indeed and places it inbto a df        \n",
    "def parse_indeed(company, destination, search1=None, search2=None):\n",
    "        c = extract(f\"https://www.indeed.com/jobs?q={search1}%20{search2}%20{company}&vjk=877ae4112485d7c7&start\", 0)\n",
    "        for i in range(0, 100, 10):\n",
    "            x = transform_2(c, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55cb1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading our nationwide data into a pandas df\n",
    "nationwide_data_analytics = []\n",
    "parse_indeed(\"Nationwide\", nationwide_data_analytics, \"Data\", \"Analytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e878188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nationwide_swe = []\n",
    "parse_indeed(\"Nationwide\", nationwide_swe, \"Software\", \"Engineer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22e070ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nationwide_data_analytics = pd.DataFrame(nationwide_data_analytics)\n",
    "nationwide_swe = pd.DataFrame(nationwide_swe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60099d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling data from wikipedia list of top 100 technology comapnies, for further data collection\n",
    "r = requests.get(\"https://en.wikipedia.org/wiki/List_of_largest_Internet_companies\")\n",
    "df_list = pd.read_html(r.text) # this parses all the tables in webpages to a list\n",
    "df = df_list[1]\n",
    "top_100 = df['Company']\n",
    "top_100.at[1]='Google'\n",
    "arr = np.array(top_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce04fb",
   "metadata": {},
   "source": [
    "### These few cells loop through our array of the top_100 tech companies by revenue in order to search for various roles that being Analytics and SWE jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f65db89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n"
     ]
    }
   ],
   "source": [
    "#processes our parse_indeed function for each, name in the top_100 companies\n",
    "all_frames = []\n",
    "\n",
    "for i in arr:\n",
    "    try:\n",
    "        parse_indeed(i, all_frames, 'Data', 'Analytics')\n",
    "    except AttributeError:\n",
    "        print('Ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3c0a536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n",
      "Ignore\n"
     ]
    }
   ],
   "source": [
    "all_swe = []\n",
    "for i in arr:\n",
    "    try:\n",
    "        parse_indeed(i, all_swe, 'Software', 'Engineer')\n",
    "    except AttributeError:\n",
    "        print('Ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69eed0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analytics_all = pd.DataFrame(all_frames)\n",
    "swe_all = pd.DataFrame(all_swe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d881282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finishing our Indeed ETL Proces by loading our data into a CSV format for later analysis\n",
    "\"\"\"\n",
    "nationwide_data_analytics.to_csv('nationwide_data_analytics.csv')\n",
    "nationwide_swe.to_csv('nationwide_swe.csv')\n",
    "data_analytics_all.to_csv('data_analytics_all.csv')\n",
    "swe_all.to_csv('swe_all.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b30546a",
   "metadata": {},
   "source": [
    "# We will now switch our focus to collecting salary data from glassdoor and various other websites, related to SWE and Data Analytics Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32df847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we must create two new functions that will process data scraped from glassdoor\n",
    "def transform_salaries(soup, destination, title = None):\n",
    "    #choosing which classes we want to extract data from \n",
    "    div = soup.find('div', class_ = 'css-14426yx eu4oa1w0')\n",
    "    div2 = soup.find('div', class_ = 'css-1b8hwn9 eu4oa1w0')\n",
    "    title = title\n",
    "    \n",
    "    #getting our data from the classes\n",
    "    salary = soup.find('span', class_ = 'cmp-SalarySummaryAverage-salary cmp-SalarySummaryAverage-salary--summary css-mfbg43 e1wnkr790')\n",
    "    company = soup.find('span', itemprop = 'name', class_ = 'css-tbayqg e1wnkr790')\n",
    "    \n",
    "    salary_data = { 'Title' : title,\n",
    "                    'Salary' : salary,  \n",
    "                    'Company': company\n",
    "                  }\n",
    "                   \n",
    "    destination.append(salary_data)\n",
    "\n",
    "def parse_salaries(company, destination, search1=None, search2=None, title = None):\n",
    "        c = extract(f\"https://www.indeed.com/cmp/{company}/salaries/{search1}-{search2}\")\n",
    "        x = transform_salaries(c, destination, title)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e28a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = extract(\"https://www.indeed.com/cmp/Nationwide-Mutual-Insurance-Company/salaries/Data-Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a52d9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "transform_salaries(x, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9669416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Nationwide Salary Data on Software Engineers, Data Analysts, Data Engineers, Data Scientists\n",
    "nationwide_salary = []\n",
    "\n",
    "parse_salaries('Nationwide Mutual Insurance Company', nationwide_salary, 'Data', 'Analyst', 'Data-Analyst')\n",
    "parse_salaries('Nationwide Mutual Insurance Company', nationwide_salary, 'Data', 'Engineer', 'Data-Engineer')\n",
    "parse_salaries('Nationwide Mutual Insurance Company', nationwide_salary, 'Data', 'Scientist', 'Data-Scientist')\n",
    "parse_salaries('Nationwide Mutual Insurance Company', nationwide_salary, 'Software', 'Engineer', 'Software-Engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "313fe996",
   "metadata": {},
   "outputs": [],
   "source": [
    "nationwide_frame = pd.DataFrame(nationwide_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55669dcf",
   "metadata": {},
   "source": [
    "### Here we are now using a new function to parse salary data from the top 100 companies in the tech space. We then save that data in various dataframes and save it to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646c12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_top100_salarires(destination, search1, search2, title):\n",
    "    for i in arr:\n",
    "        try:\n",
    "            parse_salaries(i, destination, search1, search2, title)\n",
    "        except AttributeError:\n",
    "            print('Ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a73c6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_engineer_aggr = []\n",
    "parse_top100_salarires(data_engineer_aggr, 'Data', 'Engineer', 'Data Engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eda40a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analyst_aggr = []\n",
    "parse_top100_salarires(data_analyst_aggr, 'Data', 'Analyst', 'Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20c8d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_aggr = []\n",
    "parse_top100_salarires(data_science_aggr, 'Data', 'Scientist', 'Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c53b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_aggr = []\n",
    "parse_top100_salarires(swe_aggr, 'Software', 'Engineer', 'Software Engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3029e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_engineer_all = pd.DataFrame(data_engineer_aggr).dropna()\n",
    "data_analyst_all = pd.DataFrame(data_analyst_aggr).dropna()\n",
    "data_science_all = pd.DataFrame(data_science_aggr).dropna()\n",
    "swe_all = pd.DataFrame(swe_aggr).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb438328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting our dataframes to CSV files\n",
    "\"\"\"\"\n",
    "nationwide_frame.to_csv('nationwide_sal.csv')\n",
    "data_engineer_all.to_csv('data_engineer_sal.csv')\n",
    "data_analyst_all.to_csv('data_analyst_sal.csv')\n",
    "data_science_all.to_csv('data_science_sal.csv')\n",
    "swe_all.to_csv('swe_sal.csv') \n",
    "\"\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
